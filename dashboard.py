# -*- coding: utf-8 -*-
"""
–ë–£–†–ú–ê–® ¬∑ CRM –î—ç—à–±–æ—Ä–¥ v4.2
–° AI-–∞–Ω–∞–ª–∏—Ç–∏–∫–æ–π, –º–µ—Ç—Ä–∏–∫–∞–º–∏ –ø—Ä–æ–±–ª–µ–º –∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–π —Ç–æ–≤–∞—Ä–æ–≤
"""
import os, time, math, json
from datetime import datetime, timedelta
import numpy as np
import pandas as pd
import streamlit as st
import requests

try:
    import plotly.express as px
    import plotly.graph_objects as go
except:
    px = go = None

# ============ 1. CONFIG ============
st.set_page_config(page_title="–ë–£–†–ú–ê–® ¬∑ CRM", page_icon="üüß", layout="wide")

def check_password():
    def password_entered():
        st.session_state["password_correct"] = (
            st.session_state.get("username") == "admin" and
            st.session_state.get("password") == "admin123"
        )
        st.session_state.pop("password", None)
    if st.secrets.get("DISABLE_AUTH", False):
        st.session_state["password_correct"] = True
    if not st.session_state.get("password_correct", False):
        st.markdown("### üîê –í—Ö–æ–¥ ‚Äî –ë–£–†–ú–ê–®")
        st.text_input("–õ–æ–≥–∏–Ω", key="username")
        st.text_input("–ü–∞—Ä–æ–ª—å", type="password", key="password", on_change=password_entered)
        st.stop()

check_password()

# ============ 2. SECRETS ============
def get_secret(name, default=None):
    return st.secrets.get(name) or os.getenv(name, default) or default

BITRIX24_WEBHOOK   = (get_secret("BITRIX24_WEBHOOK","") or "").strip()
PERPLEXITY_API_KEY = (get_secret("PERPLEXITY_API_KEY","") or "").strip()

# ============ 3. BITRIX24 HELPERS ============
def _bx_get(method, params=None, pause=0.4):
    url = BITRIX24_WEBHOOK.rstrip("/") + f"/{method}.json"
    out, start = [], 0
    params = dict(params or {})
    while True:
        params["start"] = start
        r = requests.get(url, params=params, timeout=30)
        data = r.json()
        res = data.get("result")
        batch = (res.get("items",[]) if isinstance(res,dict) and "items" in res else res) or []
        if not batch: break
        out.extend(batch)
        if len(batch) < 50: break
        start += 50
        time.sleep(pause)
    return out

@st.cache_data(ttl=300)
def bx_get_deals(date_from=None, date_to=None, limit=1000):
    params = {"select[]":[
        "ID","TITLE","STAGE_ID","OPPORTUNITY","ASSIGNED_BY_ID","COMPANY_ID","CONTACT_ID",
        "PROBABILITY","DATE_CREATE","DATE_MODIFY","LAST_ACTIVITY_TIME","CATEGORY_ID","BEGINDATE"
    ]}
    if date_from: params["filter[>=DATE_CREATE]"] = date_from
    if date_to:   params["filter[<=DATE_CREATE]"] = date_to
    deals = _bx_get("crm.deal.list", params)
    return deals[:limit]

@st.cache_data(ttl=300)
def bx_get_users_full():
    users = _bx_get("user.get", {})
    out = {}
    for u in users:
        depts = u.get("UF_DEPARTMENT") or []
        if isinstance(depts, str):
            depts = [int(x) for x in depts.split(",") if x]
        out[int(u["ID"])] = {
            "name": ((u.get("NAME","")+u.get("LAST_NAME","")).strip() or u.get("LOGIN","")).strip(),
            "depts": list(map(int,depts)) if depts else [],
            "active": (u.get("ACTIVE","Y")=="Y")
        }
    return out

@st.cache_data(ttl=300)
def bx_get_open_activities_for_deal_ids(deal_ids):
    out = {}
    if not deal_ids: return out
    for chunk in np.array_split(list(map(int,deal_ids)), max(1,len(deal_ids)//40+1)):
        params = {"filter[OWNER_TYPE_ID]":2,"filter[OWNER_ID]":",".join(map(str,chunk)),"filter[COMPLETED]":"N"}
        acts = _bx_get("crm.activity.list", params)
        for a in acts:
            out.setdefault(int(a["OWNER_ID"]), []).append(a)
    return out

@st.cache_data(ttl=600)
def bx_get_stage_map(stage_ids):
    sort_map, name_map = {}, {}
    if not BITRIX24_WEBHOOK or not stage_ids: return sort_map, name_map
    cats = set()
    for sid in stage_ids:
        if isinstance(sid, str) and sid.startswith("C"):
            try: cats.add(int(sid.split(":")[0][1:]))
            except: pass
    try:
        base = _bx_get("crm.status.list", {"filter[ENTITY_ID]":"DEAL_STAGE"})
        for s in base:
            sort_map[s["STATUS_ID"]] = int(s.get("SORT",5000))
            name_map[s["STATUS_ID"]] = s.get("NAME") or s["STATUS_ID"]
    except: pass
    for cid in cats:
        try:
            resp = _bx_get("crm.status.list", {"filter[ENTITY_ID]":f"DEAL_STAGE_{cid}"})
            for s in resp:
                sort_map[s["STATUS_ID"]] = int(s.get("SORT",5000))
                name_map[s["STATUS_ID"]] = s.get("NAME") or s["STATUS_ID"]
        except: continue
    return sort_map, name_map

@st.cache_data(ttl=600)
def bx_get_categories():
    try:
        cats = _bx_get("crm.category.list", {})
        return {int(c["ID"]): c.get("NAME","–í–æ—Ä–æ–Ω–∫–∞") for c in cats}
    except:
        return {}

# ============ 4. AI ANALYSIS ============
def ai_analyze_manager(manager_name, deals_summary):
    if not PERPLEXITY_API_KEY:
        return "AI-–∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω."
    prompt = f"""
–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –ø—Ä–æ–¥–∞–∂–∞–º –∏ CRM-–∞–Ω–∞–ª–∏—Ç–∏–∫–µ. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ä–∞–±–æ—Ç—É –º–µ–Ω–µ–¥–∂–µ—Ä–∞.

–ú–µ–Ω–µ–¥–∂–µ—Ä: {manager_name}
–î–∞–Ω–Ω—ã–µ: {json.dumps(deals_summary, ensure_ascii=False)}

–î–∞–π –∫—Ä–∞—Ç–∫–∏–π –∞–Ω–∞–ª–∏–∑...
"""
    data = {
        "model": "sonar-pro",
        "messages": [
            {"role": "system","content":"–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ CRM-–∞–Ω–∞–ª–∏—Ç–∏–∫–µ."},
            {"role": "user","content":prompt}
        ],
        "max_tokens":800,
        "temperature":0.3
    }
    try:
        resp = requests.post(
            "https://api.perplexity.ai/chat/completions",
            headers={"Authorization":f"Bearer {PERPLEXITY_API_KEY}"},
            json=data, timeout=30
        )
        return resp.json()["choices"][0]["message"]["content"]
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ AI-–∞–Ω–∞–ª–∏–∑–∞: {str(e)}"

# ============ 5. UTILS ============
def to_dt(x):
    try:
        ts = pd.to_datetime(x, utc=True, errors="coerce")
        if pd.isna(ts): return pd.NaT
        return ts.tz_convert(None)
    except: return pd.NaT

def days_between(later, earlier):
    a,b = to_dt(later), to_dt(earlier)
    if pd.isna(a) or pd.isna(b): return None
    return max(0,int((a-b)/pd.Timedelta(days=1)))

# ============ 6. SCORING ============
def compute_health_scores(df, open_tasks_map, stuck_days=5):
    now = to_dt(pd.Timestamp.utcnow())
    rows=[]
    for _,r in df.iterrows():
        create_dt=to_dt(r.get("DATE_CREATE"))
        last = to_dt(r.get("LAST_ACTIVITY_TIME")) or to_dt(r.get("DATE_MODIFY")) or create_dt
        begin_dt=to_dt(r.get("BEGINDATE")) or create_dt
        d_work=days_between(now,create_dt) or 0
        d_noact=days_between(now,last) or 0
        d_in_stage=days_between(now,begin_dt) or 0
        has_task=len(open_tasks_map.get(int(r["ID"]),[]))>0
        flags={
            "no_company":int(r.get("COMPANY_ID") or 0)==0,
            "no_contact":int(r.get("CONTACT_ID") or 0)==0,
            "no_tasks":not has_task,
            "stuck":d_noact>=stuck_days,
            "lost":str(r.get("STAGE_ID","")).upper().find("LOSE")>=0
        }
        score=100
        if flags["no_company"]: score-=10
        if flags["no_contact"]: score-=10
        if flags["no_tasks"]: score-=25
        if flags["stuck"]: score-=25
        if flags["lost"]: score=min(score,15)
        opp=float(r.get("OPPORTUNITY") or 0.0)
        prob=float(r.get("PROBABILITY") or 0.0)
        potential=min(100,int((opp>0)*(30+min(70,math.log10(max(1,opp))/5*70))*(0.4+prob/100*0.6)))
        rows.append({
            "ID —Å–¥–µ–ª–∫–∏":int(r["ID"]),"–ù–∞–∑–≤–∞–Ω–∏–µ":r.get("TITLE",""),
            "–ú–µ–Ω–µ–¥–∂–µ—Ä ID":int(r.get("ASSIGNED_BY_ID") or 0),
            "–≠—Ç–∞–ø ID":r.get("STAGE_ID",""),
            "–í–æ—Ä–æ–Ω–∫–∞ ID":r.get("CATEGORY_ID"),"–°—É–º–º–∞":opp,
            "–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å":prob,"–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è":create_dt,
            "–î–∞—Ç–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è":to_dt(r.get("DATE_MODIFY")),"–ü–æ—Å–ª–µ–¥–Ω—è—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å":last,
            "–ù–∞—á–∞–ª–æ —ç—Ç–∞–ø–∞":begin_dt,"–î–Ω–µ–π –≤ —Ä–∞–±–æ—Ç–µ":d_work,
            "–î–Ω–µ–π –±–µ–∑ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏":d_noact,"–î–Ω–µ–π –Ω–∞ —ç—Ç–∞–ø–µ":d_in_stage,
            "–ó–¥–æ—Ä–æ–≤—å–µ":max(0,min(100,int(score))),
            "–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª":max(0,min(100,int(potential))),
            "–ù–µ—Ç –∫–æ–º–ø–∞–Ω–∏–∏":flags["no_company"],
            "–ù–µ—Ç –∫–æ–Ω—Ç–∞–∫—Ç–∞":flags["no_contact"],
            "–ù–µ—Ç –∑–∞–¥–∞—á":flags["no_tasks"],
            "–ó–∞—Å—Ç—Ä—è–ª–∞":flags["stuck"],
            "–ü—Ä–æ–∏–≥—Ä–∞–Ω–∞":flags["lost"]
        })
    return pd.DataFrame(rows)

def focus_scores(df, horizon_days=14, min_prob=50):
    if df.empty:
        return df.assign(**{"–°–∫–æ—Ä –±—ã—Å—Ç—Ä–æ–π –ø–æ–±–µ–¥—ã":0.0, "ETA –¥–Ω–µ–π":np.nan, "–°–∫–æ—Ä –æ—Ç–∫–∞–∑–∞":0.0})
    eps=1e-9
    prob=df["–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å"].clip(0,100)/100
    health=df["–ó–¥–æ—Ä–æ–≤—å–µ"].clip(0,100)/100
    opp=df["–°—É–º–º–∞"].clip(lower=0)
    opp_norm=np.log1p(opp)/max(np.log1p(opp).max(),eps)
    smin,smax=float(df["–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ —ç—Ç–∞–ø–∞"].min()),float(df["–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ —ç—Ç–∞–ø–∞"].max())
    if smax-smin<eps:
        stage_closeness=pd.Series(0.5,index=df.index)
    else:
        stage_closeness=(df["–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ —ç—Ç–∞–ø–∞"]-smin)/(smax-smin)
    stage_closeness=np.where(df["–≠—Ç–∞–ø ID"].astype(str).str.contains("LOSE",na=False),0,stage_closeness)
    recency=1-(df["–î–Ω–µ–π –±–µ–∑ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏"].clip(lower=0)/max(horizon_days,1)).clip(0,1)
    quick=0.35*prob+0.25*health+0.15*recency+0.15*stage_closeness+0.10*opp_norm
    quick_score=(quick*100).round(1)
    eta=(30*(1-stage_closeness)*(1-0.5*health-0.5*prob)).clip(lower=0)
    eta_days=eta.round(0)
    age_norm=(df["–î–Ω–µ–π –≤ —Ä–∞–±–æ—Ç–µ"]/max(df["–î–Ω–µ–π –≤ —Ä–∞–±–æ—Ç–µ"].max(),1)).clip(0,1)
    noact_norm=(df["–î–Ω–µ–π –±–µ–∑ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏"]/max(df["–î–Ω–µ–π –±–µ–∑ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏"].max(),1)).clip(0,1)
    drop=(1-prob)*0.4+(1-health)*0.3+noact_norm*0.2+age_norm*0.1
    drop_score=(drop*100).round(1)
    out=df.copy()
    out["–°–∫–æ—Ä –±—ã—Å—Ç—Ä–æ–π –ø–æ–±–µ–¥—ã"]=quick_score
    out["ETA –¥–Ω–µ–π"]=eta_days
    out["–°–∫–æ—Ä –æ—Ç–∫–∞–∑–∞"]=drop_score
    out["–ë—ã—Å—Ç—Ä–∞—è –ø–æ–±–µ–¥–∞?"]=(quick_score>=60)&(df["–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å"]>=min_prob)&(~df["–ü—Ä–æ–∏–≥—Ä–∞–Ω–∞"])
    out["–°—Ç–æ–ø-–ª–∏—Å—Ç?"]=(drop_score>=70)|(df["–ü—Ä–æ–∏–≥—Ä–∞–Ω–∞"])|((df["–ó–¥–æ—Ä–æ–≤—å–µ"]<40)&(df["–î–Ω–µ–π –±–µ–∑ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏"]>horizon_days))
    return out

def compute_conversion_by_manager_and_funnel(df, sort_map):
    results=[]
    for (mgr,cat),g in df.groupby(["–ú–µ–Ω–µ–¥–∂–µ—Ä","–í–æ—Ä–æ–Ω–∫–∞"],dropna=False):
        stages_sorted=sorted(g["–≠—Ç–∞–ø ID"].unique(),key=lambda s:sort_map.get(str(s),9999))
        counts=g.groupby("–≠—Ç–∞–ø ID").size()
        total=len(g)
        stage_data=[]
        for s in stages_sorted:
            cnt=counts.get(s,0)
            conv=cnt/total*100 if total>0 else 0
            stage_data.append({"–≠—Ç–∞–ø":s,"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ":cnt,"–ö–æ–Ω–≤–µ—Ä—Å–∏—è %":round(conv,1)})
        results.append({"–ú–µ–Ω–µ–¥–∂–µ—Ä":mgr,"–í–æ—Ä–æ–Ω–∫–∞":cat,"–í—Å–µ–≥–æ —Å–¥–µ–ª–æ–∫":total,"–≠—Ç–∞–ø—ã":stage_data})
    return pd.DataFrame(results)

# ============ 7. SIDEBAR ============
st.sidebar.title("–§–∏–ª—å—Ç—Ä—ã")
date_from=st.sidebar.date_input("–° –¥–∞—Ç—ã",datetime.now().date()-timedelta(days=30))
date_to=st.sidebar.date_input("–ü–æ –¥–∞—Ç—É",datetime.now().date())
stuck_days=st.sidebar.slider("–ù–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ ‚â• (–¥–Ω–µ–π)",2,21,5)
limit=st.sidebar.slider("–õ–∏–º–∏—Ç —Å–¥–µ–ª–æ–∫ (API)",50,3000,600,step=50)
st.sidebar.title("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Ñ–æ–∫—É—Å–∞ –†–û–ü–∞")
focus_horizon=st.sidebar.slider("–ì–æ—Ä–∏–∑–æ–Ω—Ç —Ñ–æ–∫—É—Å–∞ (–¥–Ω–µ–π)",7,45,14)
focus_min_prob=st.sidebar.slider("–ú–∏–Ω. –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–ª—è —Ñ–æ–∫—É—Å–∞, %",0,100,50)

# ============ 8. LOAD DATA ============
with st.spinner("–ó–∞–≥—Ä—É–∂–∞—é –¥–∞–Ω–Ω—ã–µ‚Ä¶"):
    if not BITRIX24_WEBHOOK:
        st.error("–ù–∞—Å—Ç—Ä–æ–π—Ç–µ BITRIX24_WEBHOOK –≤ Secrets")
        st.stop()
    deals_raw=bx_get_deals(str(date_from),str(date_to),limit=limit)
    if not deals_raw:
        st.error("–°–¥–µ–ª–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
        st.stop()
    df_raw=pd.DataFrame(deals_raw)
    df_raw["OPPORTUNITY"]=pd.to_numeric(df_raw.get("OPPORTUNITY"),errors="coerce").fillna(0.0)
    users_full=bx_get_users_full()
    users_map={uid:users_full[uid]["name"] for uid in users_full}
    open_tasks_map=bx_get_open_activities_for_deal_ids(df_raw["ID"].tolist())
    categories_map=bx_get_categories()

df_scores=compute_health_scores(df_raw,open_tasks_map,stuck_days)
stage_ids=df_scores["–≠—Ç–∞–ø ID"].dropna().unique().tolist()
sort_map,name_map=bx_get_stage_map(stage_ids)
FALLBACK_ORDER=["NEW","NEW_LEAD","PREPARATION","PREPAYMENT_INVOICE","EXECUTING","FINAL_INVOICE","WON","LOSE"]
df_scores["–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ —ç—Ç–∞–ø–∞"]=df_scores["–≠—Ç–∞–ø ID"].map(lambda s:sort_map.get(str(s),FALLBACK_ORDER.index(str(s).split(":")[-1])*100 if str(s).split(":")[-1] in FALLBACK_ORDER else 10000))
df_scores["–ù–∞–∑–≤–∞–Ω–∏–µ —ç—Ç–∞–ø–∞"]=df_scores["–≠—Ç–∞–ø ID"].map(lambda s:name_map.get(str(s),str(s)))
df_scores["–ú–µ–Ω–µ–¥–∂–µ—Ä"]=df_scores["–ú–µ–Ω–µ–¥–∂–µ—Ä ID"].map(users_map).fillna("–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ")
df_scores["–í–æ—Ä–æ–Ω–∫–∞"]=df_scores["–í–æ—Ä–æ–Ω–∫–∞ ID"].map(lambda x:categories_map.get(int(x or 0),"–û—Å–Ω–æ–≤–Ω–∞—è"))
df_scores=focus_scores(df_scores,focus_horizon,focus_min_prob)

funnels=sorted(df_scores["–í–æ—Ä–æ–Ω–∫–∞"].unique())
selected_funnels=st.sidebar.multiselect("–í–æ—Ä–æ–Ω–∫–∏",funnels,default=funnels)
managers=sorted(df_scores["–ú–µ–Ω–µ–¥–∂–µ—Ä"].unique())
selected_managers=st.sidebar.multiselect("–ú–µ–Ω–µ–¥–∂–µ—Ä—ã",managers,default=managers)

view_df=df_scores[df_scores["–í–æ—Ä–æ–Ω–∫–∞"].isin(selected_funnels)&df_scores["–ú–µ–Ω–µ–¥–∂–µ—Ä"].isin(selected_managers)]
if view_df.empty:
    st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º —Ñ–∏–ª—å—Ç—Ä–∞–º")
    st.stop()

# ============ 9. HEADER ============
st.markdown("# üüß –ë–£–†–ú–ê–® ¬∑ CRM –î—ç—à–±–æ—Ä–¥ v4.2")
st.markdown(f"**–ü–µ—Ä–∏–æ–¥**: {date_from} ‚Üí {date_to} | **–°–¥–µ–ª–æ–∫**: {len(view_df)}")

# ============ 10. MAIN TABS ============
tab1,tab2,tab3,tab4,tab5,tab6=st.tabs(["üìä –û–±–∑–æ—Ä","‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º—ã","üë§ –ü–æ –º–µ–Ω–µ–¥–∂–µ—Ä–∞–º","üéØ –ì—Ä–∞–¥–∞—Ü–∏—è —Å–¥–µ–ª–æ–∫","‚è±Ô∏è –í—Ä–µ–º—è –Ω–∞ —ç—Ç–∞–ø–∞—Ö","ü§ñ AI-–∞–Ω–∞–ª–∏—Ç–∏–∫–∞"])

# -- TAB1 --
with tab1:
    c1,c2,c3,c4=st.columns(4)
    c1.metric("–°–¥–µ–ª–æ–∫",len(view_df),key="m1")
    c2.metric("–í—ã—Ä—É—á–∫–∞, ‚ÇΩ",f"{int(view_df['–°—É–º–º–∞'].sum()):,}",key="m2")
    c3.metric("–°—Ä. –∑–¥–æ—Ä–æ–≤—å–µ",f"{int(view_df['–ó–¥–æ—Ä–æ–≤—å–µ'].mean())}%",key="m3")
    c4.metric("–°—Ä. –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª",f"{int(view_df['–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª'].mean())}%",key="m4")
    if px:
        fig1=px.histogram(view_df,x="–ó–¥–æ—Ä–æ–≤—å–µ",nbins=20,title="–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–¥–æ—Ä–æ–≤—å—è",labels={"–ó–¥–æ—Ä–æ–≤—å–µ":"–ó–¥–æ—Ä–æ–≤—å–µ (%)"},color_discrete_sequence=["#FF6B35"])
        st.plotly_chart(fig1,use_container_width=True,key="fig1")
    # ‚Ä¶ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –¥–ª—è fig2(fig_key="fig2"), fig3("fig3")

# -- TAB2 --
with tab2:
    # ‚Ä¶ –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–±–ª–µ–º –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
    if px:
        problem_counts=pd.DataFrame({...})
        fig_prob=px.bar(problem_counts,...) 
        st.plotly_chart(fig_prob,use_container_width=True,key="fig_prob")

# -- TAB3 --
with tab3:
    # ‚Ä¶ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –º–µ–Ω–µ–¥–∂–µ—Ä–æ–≤
    if px and not df_mgr.empty:
        fig4=px.bar(df_mgr,...) 
        st.plotly_chart(fig4,use_container_width=True,key="fig4")
        fig5=px.scatter(df_mgr,...)
        st.plotly_chart(fig5,use_container_width=True,key="fig5")
    conv_data=compute_conversion_by_manager_and_funnel(view_df,sort_map)
    for i,row in conv_data.iterrows():
        with st.expander(f"üë§ {row['–ú–µ–Ω–µ–¥–∂–µ—Ä']} | {row['–í–æ—Ä–æ–Ω–∫–∞']} ({row['–í—Å–µ–≥–æ —Å–¥–µ–ª–æ–∫']})",key=f"exp_m{i}"):
            stage_df=pd.DataFrame(row["–≠—Ç–∞–ø—ã"])
            st.dataframe(stage_df,use_container_width=True)
            if px and not stage_df.empty:
                fig6=px.funnel(stage_df,x="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ",y="–≠—Ç–∞–ø",title="–í–æ—Ä–æ–Ω–∫–∞ –∫–æ–Ω–≤–µ—Ä—Å–∏–∏")
                st.plotly_chart(fig6,use_container_width=True,key=f"fig6_{i}")

# -- TAB4, TAB5, TAB6 –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ: –≤—Å–µ–º plotly_chart –∏ expander –¥–æ–±–∞–≤–∏—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ key --

# ===== TAB 7: –¢–û–í–ê–†–´ =====
tab_products=st.tabs(["üì¶ –¢–æ–≤–∞—Ä—ã"])[0]
with tab_products:
    st.subheader("–ê–Ω–∞–ª–∏—Ç–∏–∫–∞ —Ç–æ–≤–∞—Ä–æ–≤ –≤ —Å–¥–µ–ª–∫–∞—Ö")
    @st.cache_data(ttl=300)
    def bx_get_deal_products(deal_id):
        try: return _bx_get("crm.deal.productrows.get",{"id":deal_id}) or []
        except: return []
    def load_all_products(view_df):
        recs=[]
        for deal_id in view_df["ID —Å–¥–µ–ª–∫–∏"].unique():
            for p in bx_get_deal_products(deal_id):
                recs.append({...})
        return pd.DataFrame(recs)
    df_products=load_all_products(view_df)
    if df_products.empty:
        st.info("–ù–µ—Ç —Ç–æ–≤–∞—Ä–Ω—ã—Ö –ø–æ–∑–∏—Ü–∏–π")
    else:
        agg=(df_products.groupby("product_name"){"quantity":"sum","total":"sum"}
             .rename(...).reset_index())
        st.dataframe(agg,use_container_width=True)
        stock_raw=_bx_get("crm.catalog.store.product.list",{})
        df_stock=pd.DataFrame(stock_raw).rename(...).astype(...)
        last_30=datetime.now()-timedelta(days=30)
        recent_deals=df_raw[...] 
        recent_products=load_all_products(recent_deals)
        avg_daily=(recent_products.groupby("product_id")["quantity"].sum()/30).to_dict()
        recs2=[]
        for j,row in agg.iterrows():
            pid=...
            stock=...
            avg=avg_daily.get(pid,0)
            to_order=max(0,avg*14-stock)
            recs2.append({...})
        df_rec=pd.DataFrame(recs2).sort_values(... )
        st.subheader("–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∑–∞–∫—É–ø–∫–µ")
        st.dataframe(df_rec,use_container_width=True,key="df_rec")

st.markdown("---")
st.caption("–ë–£–†–ú–ê–® ¬∑ CRM –î—ç—à–±–æ—Ä–¥ v4.2 | Powered by Bitrix24 + Perplexity AI")
